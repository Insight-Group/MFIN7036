# Data Visulization 
Followed by Data Reshaping and Preprocessing, we obtained a series of tokenized words. Due to the large amount of data set, it is hard for us to uderstand the underlying trends and patterns. Therefore, the data visulization becomes neccesary. As a part of data visulization, the frequency table and wordclouds are genenarated to tell the story behind the data.   

## 1. Process
### 1.1 Generating Frequency Set 
Firstly, we use dictionary in Python to count the frequencies in the 'final_token' list obtained from data shaping. and increment the counter using loops. 
```python
import matplotlib.pyplot as plt
from nltk.corpus import stopwords

frequency = dict()

df=pd.DataFrame(data_frame)
final_token=df.final_token

def word_count(final_token):
    for token in final_token:
        for word in token:
            if word in frequency:
                frequency[word]+=1
            else:
                frequency[word]=1
    return frequency
    
word_count(final_token)

print(frequency)
```
Note:The Def in python is short for "define", which performs a specific task. Therefore, the statements under Def should run together. 

### 1.2 Word Clouds
Word clouds  are visual representations of words. Here our geoup use word clouds to popular words based on word frequency. In this way, the large amount of data could be present in a quick and simple visual insights. 
```python 
from wordcloud import WordCloud, STOPWORDS, ImageColorGenerator
from PIL import Image
import os
import numpy as np

text = " ".join([(k+" ")*v for k,v in frequency.items()])

stopwords = set(STOPWORDS)

path = "/Users/luqilin/MFIN7036-Blog/code/red-white-pill-hi.png"

pill = np.array(Image.open(path))

wc = WordCloud(stopwords = stopwords,mask = pill, background_color="white", width = 30000, height = 20000,collocations = False,max_words=50)

wc.generate(text)

plt.imshow(wc, interpolation='bilinear')

plt.axis("off")

plt.show
```








