# Data Reshaping and Preprocessing

The downloaded tweets are barely basic raw data. To use it for further research, we build some more columns based on it with python.

## 1 Tokenization

Tokenization is the process of splitting up a text into smaller parts, also called BoW(bag of word). In our case, we processed on the `Text` column of our downloaded tweets CSV file and generated the `final_token` column with tokens split and filtered from the `Text` column.

```python
def tokenize_and_add_column(twitter_data_frame):
    wl = WordNetLemmatizer()
    final_tokens_list = []

    # Loop Text column of every line, which is the content of each Tweet
    for line in twitter_data_frame['Text']:
        # Genereate tokens from raw content
        tokens = word_tokenize(line)
        # Filter non-english words
        english_words = [word for word in tokens if word.isalpha()]
        # Filter stop words like for, do, an, etc
        tags = nltk.pos_tag(english_words)
        final_tokens = [word for word, pos in tags if not word.lower() in set(stopwords.words('english'))]
        final_tokens = [wl.lemmatize(word, pos="n") for word in final_tokens]
        final_tokens = [wl.lemmatize(word, pos="v") for word in final_tokens]
    
        final_tokens_list.append(final_tokens)
    
    # Add final_token column for each line
    twitter_data_frame['final_token'] = pd.Series(final_tokens_list)
    
    print("*****************************")
    print('Print example tokenized twitter text of first row')
    print(twitter_data_frame['final_token'][0])
    return twitter_data_frame
```

## 2 TF-IDF Preprocessing

After Tokenization, we get a list of BoW, but all words in it are not equal. Some words appear many times in a tweet but do not add any semantic meaning to the text like `the` and some other concept words like `computer`. We have removed some of these kinds of words in the last step, but some of them remain. The TF-IDF model is used to measure the importance of each word.

```python
def calculate_tfidf_and_add_column(twitter_data_frame):
    # Get words set of all tweets
    words = []
    for line in twitter_data_frame['final_token']:
        words.extend(line)
    uniquewords = set(words)
    
    all_number_of_words = []
    all_tf = []
    # Loop final_token column of every line, which is generated by tokenize_and_add_column method
    for line in twitter_data_frame['final_token']:
        # Count words exists
        numberOfWords = dict.fromkeys(set(line), 0)
        for word in line:
            numberOfWords[word] += 1
        all_number_of_words.append(numberOfWords)

        # Calculate term frequency of tokens of this tweet
        tf = computeTF(numberOfWords, line)
        all_tf.append(tf)
    
    # Calculate Inverse Data Frequency of each word exists in all tweets
    idfs = computeIDF(all_number_of_words, uniquewords)
    
    tfidfs = []
    # Calculat tfdif of every tweet
    for tf in all_tf:
        tfidfs.append(computeTFIDF(tf, idfs))
    # Add tfdif column for each line
    twitter_data_frame['tfidf'] = pd.Series(tfidfs)
        
    print("*****************************")
    print('Print example tfidf of first row')
    print(twitter_data_frame['tfidf'][0])
    return twitter_data_frame
```

## 3 How to Use

### Step 1 Clone Repository

- git clone https://github.com/Insight-Group/MFIN7036-Blog.git

and then, change to the project root directory

- cd MFIN7036-Blog

### Step 2 Downloade Dependencies

- pip install -r ./code/requirements.txt

There is one step further. Nltk package needs some attached data to run it, simply run:

- python ./code/download_nltk_data.py

If error messages emerge like `remote server shuts down a connection`, try again with a proxy option:

- python ./code/download_nltk_data.py --proxy=https://127.0.0.1:7890

### Step 3 Run Example Code

Simply run:

- python ./code/example.py

and see logs to observe things that have happened.

You can change the file path in `example.py` to process other files.

## 4 Reference

- [How To Perform Sentiment Analysis in Python 3 Using the Natural Language Toolkit (NLTK)](https://www.digitalocean.com/community/tutorials/how-to-perform-sentiment-analysis-in-python-3-using-the-natural-language-toolkit-nltk)
- [TFIDF Python Example](https://towardsdatascience.com/natural-language-processing-feature-engineering-using-tf-idf-e8b9d00e7e76)
- [WordBricks - Text Analysis of FOMC Meeting Minutes to Predict Change in VIX](https://wordbricksfina4350.blogspot.com/2020/11/section-5-sentiment-analysis-data.html)
