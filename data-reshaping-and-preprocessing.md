# Data Reshaping and Preprocessing

The downloaded tweets are barely basic raw data. To use it for further research, we build some more columns based on it with python.

## 1 Tokenization

Tokenization is the process of splitting up a text into smaller parts, also called BoW(bag of word). In our case, we processed on the `Text` column of our downloaded tweets CSV file and generated the `word_tokens`, `sent_tokens` and `processed_text` columns with tokens splitted and filtered from the `Text` column.

```python
def tokenize_and_add_column(twitter_data_frame):
    wl = WordNetLemmatizer()
    word_tokens_column = []
    sent_tokens_column = []
    processed_text_column = []

    # Loop Text column of every tweet, which is the content of each Tweet
    for tweet in twitter_data_frame['Text']:
        # Remove urls
        tweet = re.sub(r"http\S+|www\S+|https\S+", '', tweet, flags=re.MULTILINE)
        # Remove user @ references and '#' from tweet
        tweet = re.sub(r'\@\w+|\#','', tweet)
        # Remove punctuations
        tweet = tweet.translate(str.maketrans('', '', string.punctuation))
        # Generate tokens
        words = word_tokenize(tweet)
        sents = sent_tokenize(tweet)
        # Filter out non-english words
        english_words = [word for word in words if word.encode().isalpha()]
        # Filter out stopwords like for, do, an, etc
        tags = nltk.pos_tag(english_words)
        nonstop_words = [word for word, pos in tags if not word.lower() in set(stopwords.words('english'))]
        word_tokens = [wl.lemmatize(word, get_wordnet_pos(word)) for word in nonstop_words]
    
        word_tokens_column.append(word_tokens)
        sent_tokens_column.append(sents)
        
        if len(word_tokens) == 0:
            processed_text_column.append("without useful information")
        else:
            processed_text_column.append(" ".join(word_tokens))
    
    # Add columns for each tweet - three types of processed data
    twitter_data_frame['word_tokens'] = pd.Series(word_tokens_column)        
    twitter_data_frame['sent_tokens'] = pd.Series(sent_tokens_column)
    twitter_data_frame['processed_text'] = pd.Series(processed_text_column)
    
    print("*****************************")
    print('Print example tokenized twitter text of first row')
    print(twitter_data_frame['final_token'][0])

    return twitter_data_frame

def get_wordnet_pos(word):
    """Map POS tag to first character lemmatize() accepts"""
    tag = nltk.pos_tag([word])[0][1][0].upper()
    tag_dict = {"J": wordnet.ADJ,
                "N": wordnet.NOUN,
                "V": wordnet.VERB,
                "R": wordnet.ADV}
    # if the pos_tag of the word cannot be matched in tag_dict, then define it as the defalut 'wordnet.NOUN'
    return tag_dict.get(tag, wordnet.NOUN) 
```

## 2 TF-IDF Preprocessing

After Tokenization, we get a list of BoW, but all words in it are not equal. Some words appear many times in a tweet but do not add any semantic meaning to the text like `the` and some other concept words like `computer`. We have removed some of these kinds of words in the last step, but some of them remain. The TF-IDF model is used to measure the importance of each word.

```python
def calculate_tfidf_and_add_column(twitter_data_frame):
    # Get words SET of all tweets
    words = []
    for tweet in twitter_data_frame['word_tokens']:
        words.extend(tweet)
    uniquewords = set(words)
    
    all_number_of_words = []
    all_tf = []
    # Loop word_tokens column of every tweet, which is generated by tokenize_and_add_column method
    for tweet in twitter_data_frame['word_tokens']:
        # Count words exists
        numberOfWords = dict.fromkeys(set(tweet), 0)
        for word in tweet:
            numberOfWords[word] += 1
        all_number_of_words.append(numberOfWords)

        # Calculate term frequency of tokens of this tweet
        tf = computeTF(numberOfWords, tweet)
        all_tf.append(tf)
    
    # Calculate Inverse Data Frequency of each word exists in all tweets
    idfs = computeIDF(all_number_of_words, uniquewords)
    
    tfidfs = []
    # calculate tfdif of every tweet
    for tf in all_tf:
        tfidfs.append(computeTFIDF(tf, idfs))
    # Add tfdif column for each tweet, tfdif is the list of dict
    twitter_data_frame['tfidf'] = pd.Series(tfidfs)
        
    print("*****************************")
    print('Print example tfidf of first row')
    print(twitter_data_frame['tfidf'][0])

    return twitter_data_frame
```

## 3 How to Use

### Step 1 Clone Repository

- git clone https://github.com/Insight-Group/MFIN7036-Blog.git

and then, change to the project root directory

- cd MFIN7036-Blog

### Step 2 Downloade Dependencies

- pip install -r ./code/requirements.txt

There is one step further. Nltk package needs some attached data to run it, simply run:

- python ./code/download_nltk_data.py

If error messages emerge like `remote server shuts down a connection`, try again with a proxy option:

- python ./code/download_nltk_data.py --proxy=https://127.0.0.1:7890

### Step 3 Run Example Code

Simply run:

- python ./code/example.py

and see logs to observe things that have happened.

You can change the file path in `example.py` to process other files.

## 4 Reference

- [How To Perform Sentiment Analysis in Python 3 Using the Natural Language Toolkit (NLTK)](https://www.digitalocean.com/community/tutorials/how-to-perform-sentiment-analysis-in-python-3-using-the-natural-language-toolkit-nltk)
- [TFIDF Python Example](https://towardsdatascience.com/natural-language-processing-feature-engineering-using-tf-idf-e8b9d00e7e76)
- [WordBricks - Text Analysis of FOMC Meeting Minutes to Predict Change in VIX](https://wordbricksfina4350.blogspot.com/2020/11/section-5-sentiment-analysis-data.html)
